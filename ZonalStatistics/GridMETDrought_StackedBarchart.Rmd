---
title: 'Generating stacked barcharts from GridMET Drought data'
author: "Eric Jensen"
date: "2024-03-04"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preliminaries

### Purpose of this notebook
In this notebook we'll produce two summary visualizations for drought data from the gridMET Drought dataset, 1) a stacked barchart of drought categoies for the state of Colorado using the 180-day standardized precipitation-evapotranspiration index (SPEI) as a drought indicator and 2) summary tables providing the average and maximum amount of the state of Colorado in each drought category throughout the year.

Find more information about gridMET Drought here: https://support.climateengine.org/article/45-gridmet-drought

```{r}
# Import libraries
library(tidyverse) # because tidyverse is the best <3
library(sf) # for processing spatial data
library(httr2) # for Climate Engine API requests
library(mapview) # for visualizing spatial data
library(spData) # for US states shapefile
```


## Authenticate to API and run preliminary test

Define Climate Engine API key and run a simple test endpoint to make sure that the key authenticated correctly. The Climate Engine API documentation is published at https://docs.climateengine.org

The Climate Engine API has a 'Swagger' app for testing and making requests at: https://api.climateengine.org/docs

To request an API key, contact the Climate Engine team at support.climateengine.org.

```{r}
# Define root url for Climate Engine API
root_url <- 'https://api.climateengine.org/'

# Define endpoint
endpoint <- '/home/validate_key'

# Define key
key <- 'key_here'

# Run simple endpoint to get key expiration and print result
test <- request(base_url = paste0(root_url, endpoint)) |>
  req_headers(Authorization = key) |>
  req_perform()
print(resp_raw(test))

# Clean up unneeded objects
rm(test, endpoint)
```


## Import US states data and select Colorado as Area of Interest

This section of the notebook runs using the state of Colorado, but you can run similar analysis for any polygon shapefile (if you are using multipolygons, you will need to modify the next step by parsing coordinates slightly differently). We will import the us_states layer from the spData package as an sf object and then subset for the state of Colorado and visualize the it on a leaflet map using the mapview package. 

```{r}
# Import us_states from spData package
data(us_states)

# Subset us_states for Colorado
aoi <- us_states |>
  filter(NAME == 'Colorado')

mapview(aoi)
rm(us_states)
```


### Parse coordinates from Colorado AOI to pass to API endpoint

Here we will extract the coordinates from the AOI polygon (Colorado) and parse it into a list of coordinates that the API will recognize. The coordinate pattern used by the Climate Engine API is consistent with the RFC 7946 standard and is the standard for the GeoJSON file format.

```{r}
# Get coordinates of polygon and parse to list of coordinates
coords_matrix <- st_coordinates(aoi)
x <- coords_matrix[,1]
y <- coords_matrix[,2]
coords_str <- as.character()
for (i in seq(1, length(x))){
  coords_str <- paste0(coords_str, '[', as.character(x[i]), ',', as.character(y[i]), '],')
}
coords_str <- paste0('[[', str_sub(coords_str, 1, -2), ']]')

rm(coords_matrix, x, y, i)
```


## Fetch drought data from gridMET Drought.

Here, we will request histogram data for the 180-day standardized precipitation-evapotranspiration index (SPEI) using the 'zonal_stats/pixel_count/polygons' API endpoint. More information: https://docs.climateengine.org/docs/build/html/zonal_statistics.html#rst-zonal-stats-pixel-count-custom-asset The bins used in the request are based on on drought categories developed by the National Drought Mitigation Center (NDMC) (https://droughtmonitor.unl.edu/About/AbouttheData/DroughtClassification.aspx). 

You can learn more about drought indicators and how the Climate Engine team is helping the Bureau of Land Management to assess drought impacts on the Climate Engine Support Site: https://support.climateengine.org/article/128-rangeland-drought-assessment

NOTE: It may take about 5-10 minutes to retrieve all of the data.

```{r}
# Generate a date list to extract drought statistics over
start_year = 2020
end_year = 2023
yearlist = seq(from = start_year, to = end_year, by = 1)

# Loop over the yearlist to generate a list of dates for each year from 2015-2023
dates = list()
for (year in yearlist){
  dates_year <- seq(from = as.Date(paste0(as.character(year), "/1/5")), to = as.Date(paste0(as.character(year), "/12/31")), by = '5 days')
  dates <- c(dates, as.character(dates_year))
  }
rm(year, yearlist, dates_year, end_year, start_year)

# Documentation on the gridMET Drought is available here: https://support.climateengine.org/article/45-gridmet-drought
# API Documentation on the gridMET Drought: https://docs.climateengine.org/docs/build/html/variables.html#rst-gridmet-drought
# Generate dataframe of histograms
get_drought_histogram <- function(date){

  # print(paste0('Running ', date))

  # Create list of API parameters for request
  params = list(
    dataset = 'GRIDMET_DROUGHT',
    variable = 'spei180d',
    end_date = as.character(date),
    area_reducer = 'mean',
    coordinates = coords_str,
    bins = '[-2, -1.5, -1.2,-0.7, -0.5, 0.5, 0.7, 1.2, 1.5, 2]')

  # Make API request and get data from response
  data <- request(base_url = paste0(root_url, 'zonal_stats/pixel_count/polygons')) |>
    req_url_query(query = !!!params) |>
    req_headers(Authorization = key) |>
    req_perform() |>
    resp_body_json() |>
    pluck(1)

  # Iterate over the SPEI histogram bins to produce dataframe of SPEI histograms
  bins = names(data$spei180d)
  row_df <- tibble('Date' = date)
  for (bin in bins){
    val = data$spei180d[bin][1] |>
      unlist() |>
      unname()
    df = tibble(bin = val)
    names(df) <- bin
    row_df = bind_cols(row_df, df)}

  return(row_df) }

spei_df <- purrr::map(dates, get_drought_histogram) |>
  bind_rows() %>%
  mutate(across(everything(), replace_na, 0))
```

### Prepare the drought data for plotting

In the code below we will prepare the 180-day SPEI drought dataframe that we produced in the last section for visualization. First, we will pivot the dataframe from wide to long; then, we will calculate the percentage of the area of interest that is in each drought category for each date; last, we will order the categories in the correct order to match the figure referenced at the beginning of this case study.

```{r}
# Read in the 180-day SPEI data CSV
# names(spei_df) <- c("Date", "1",	"2",	"3",	"4",	"5",	"0",	"-1",	"-2",	"-3",	"-4",	"-5",	"null")
# Clean and prepare the data for plotting the stacked bar chart
spei_df_plot <- spei_df |> 
  mutate(Date = as.Date(Date)) |> # The date needs to be a date object for the plot
  pivot_longer(!Date, names_to = 'Category', values_to = 'Count') |> # pivot from wide to long dataframe for plotting
  group_by(Date) |>
  mutate(Pixels = sum(Count)) |> # calculate sum of pixels for each date for generating histogram
  ungroup() |>
  mutate(Category = factor(Category, levels = c('0', '-1', '-2', '-3', '-4', '-5', '1', '2', '3', '4', '5'))) |> # convert Drought Category to a factor for plotting
  mutate(Percent = (Count/Pixels) * 100) |> # calculate percent of pixels in each class for generating histogram
  filter(Category %in% c('0', '-1', '-2', '-3', '-4', '-5')) # filter for drought categories
spei_df_plot
```


### Plot the stacked barchart for the drought data in ggplot2

The code below uses the dataframe that we generated in the previous step and applies visualization paramters and drought labels consistent with those used by the National Drought Mitigation Center (NDMC).

``` {r warning = FALSE}
# make a figure to display SPEI in the sandy ESG
spei_plot <- ggplot(spei_df_plot, mapping = aes(x = Date, y = Percent, fill = Category))+
            geom_bar(stat = 'identity', width = 10)+
             scale_fill_manual(values = c('0' = '#FFFFFF', '-1' = '#FEFE33', '-2' = '#FFD580', '-3' = '#FFA500', '-4' = '#DC143C', '-5' = '#8C000F'), # define drought category colors according to NDMC: https://droughtmonitor.unl.edu/About/AbouttheData/DroughtClassification.aspx
                              labels = c('No drought', 'Abnormally\ndry', 'Moderate', 'Severe', 'Extreme', 'Exceptional'))+ # define drought category names according to NDMC: https://droughtmonitor.unl.edu/About/AbouttheData/DroughtClassification.aspx
            scale_y_continuous(breaks=c(0, 20, 40, 60, 80, 100), expand = c(0, 0)) +
  scale_x_date(expand = c(0, 0)) +  # the expand gets rid of gaps between x and y axis lines and the data 
            labs(title = '180-day SPEI Timeseries Plot for Colorado', x = '', y = 'Percent', fill = 'Drought\nCategory')+
            theme_classic()+
            theme(legend.key=element_rect(colour="grey"))

spei_plot
```

## Create summary tables of drought data

The code below manipulates the drought histogram data that we received from the API to calculate the percentage of Colorado in each drought category (or greater) for each date in the dataset.

For reference:
D0 = Abnormally Dry	
D1 = Moderate Drought
D2 = Severe Drought
D3 = Extreme Drought
D4 = Exceptional Drought

```{r}
# Clean and prepare data for summary table
spei_df_table <- spei_df |>
  mutate(Pixels = rowSums(across(where(is.numeric)))) |> # calculate total pixels in each row
  select('Date', 'D0' = '-1', , 'D1' = '-2', 'D2' = '-3', , 'D3' = '-4', 'D4' = '-5', 'Pixels') |> # remove non-drought columns
  mutate(D0 = ((D0 + D1 + D2 + D3 + D4) / Pixels) * 100,
         D1 = ((D1 + D2 + D3 + D4) / Pixels) * 100,
         D2 = ((D2 + D3 + D4) / Pixels) * 100,
         D3 = ((D3 + D4) / Pixels) * 100,
         D4 = ((D4) / Pixels) * 100) |> # calculate new columns for the total area in class or in more severe class as percentage
  mutate(Year = as.numeric(stringr::str_extract(Date, "^\\d{4}"))) # create new column for year to group_by
spei_df_table
```

The code below groups the data by each year in the dataset and then uses the summarize_at function to produce summary tables of 1) the average percentage of Colorado in each drought category for each year and 2) the maximum percentage of Colorado in each drought category for each year.

``` {r}  
# Get average area in each drought category (or greater) during each year and round to two decimals
spei_df_mean_table <- spei_df_table|>
  group_by(Year) |> # calculate maximum and average areas in drought by year for each category
  summarize_at(c('D0', 'D1', 'D2', 'D3', 'D4'), ~round(mean(.), 2))
spei_df_mean_table

# Get maximum area in each drought category (or greater) during each year and round to two decimals
spei_df_max_table <- spei_df_table |>
  group_by(Year) |> # calculate maximum and average areas in drought by year for each category
  summarize_at(c('D0', 'D1', 'D2', 'D3', 'D4'), ~round(max(.), 2))
spei_df_max_table
```
